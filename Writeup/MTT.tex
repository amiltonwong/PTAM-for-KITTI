\documentclass[10pt,column,letterpaper]{article}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\begin{document}

\title{Status Report for Parallel Tracking and Mapping for Outdoor Exploration} % Replace with your title

\author{Daniel DeTone, Yu Xiang and Silvio Savarese \\
Department of Computer Science and Electrical Engineering\\
University of Michigan at Ann Arbor\\
{\tt\small \{ddetone, yuxiang, silvio\}@eecs.umich.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Silvio Savarese\\
%Department of Computer Science and Electrical Engineering\\
%University of Michigan\\
%Ann Arbor, MI 48109 \\
%{\tt\small silvio@eecs.umich.edu}
}


\maketitle

\begin{abstract}


\end{abstract}

\section{Overview}

This report describes modifications done to the work of Klein and Murray [1] for operation in outdoor exploration, rather than in contained, small workspaces.  The major difference between outdoor exploration and indoor workspaces is that once an observed map feature leaves the field of view in an outdoor exploration setting, it has a very low chance of being reobserved. The purpose of the modification is to use the framework provided by [1] for accurate Simultaneous Localization and Mapping, rather than Augmented Reality. 

There are two major modifications that were made to ensure robust operation of PTAM on outdoor exploration:
\begin{itemize}
\item Only use map points from recent keyframes in the point-based tracking
\item Flag non-recent keyframes as ``fixed'' in the global bundle adjustment process
\end{itemize}
These two modifications provide the basis for the changes described below, and allow for operation of the PTAM system in many outdoor environments. The system has been tested with the KITTI dataset.


\section{3D Multi-target Tracking}

Our primary goal of 3D multi-target tracking is to estimate the posterior distribution $P(X_t | I^t)$ of the state of the 3D scene $X_t$ at the current time step $t$ given all the observations $I^t = \{ I_1, \ldots, I_t \}$ up to that time step. In our framework, the state of the 3D scene is represented by $X_t = \{ C_t, O_t, G_t \}$, where $C_t$ denotes the state of the camera, $O_t = \{ O_{1t}, \ldots, O_{Mt} \}$ represents a set of $M$ objects in the 3D world and $G_t$ denotes the ground plane in the 3D world. The number of objects can vary across frames since objects enter or exit the field of view of camera. By modeling the ground plane in the 3D world, we are able to introduce physical constraints such as ``objects should lie on the ground plane'' to achieve robust object tracking. By applying the Bayesian rule, the posterior distribution can be decomposed as
\begin{align}\label{eq:posterior}
    & P(X_t | I^t) \propto \\ & P(I_t | X_t, I^{t-1}) \int P(X_t | X_{t-1}) P(X_{t-1} | I^{t-1}) dX_{t-1}, \nonumber
\end{align}
where the likelihood $P(I_t | X_t, I^{t-1})$ measures the probability of observing the measurement $I_t$ given the state $X_t$ at time $t$ and the previous observations $I^{t-1}$, the motion model $P(X_t | X_{t-1})$ predicts the state of the scene at time $t$ given its previous state.

\subsection{Likelihood Model}

The likelihood model $P(I_t | X_t, I^{t-1})$ measures the compatibility between the scene state $X_t$ and the previous observations $I^{t-1}$ with the current observation $I_t$. According to the scene components in our framework, we decompose the likelihood as
\begin{align}\label{eq:likelihood}
     & P(I_t | X_t, I^{t-1})  =  P(I_t | C_t, O_t, G_t, I^{t-1}) \\
    & \propto P(I_t | C_t, I^{t-1}) P(I_t | C_t, O_t) P(I_t | C_t, G_t). \nonumber 
\end{align}
Here, $P(I_t | C_t, I^{t-1})$ is the likelihood of the camera given all the observations up to time $t$, which can be evaluated using Structure from Motion (SfM) techniques or visual odometry techniques. $P(I_t | C_t, O_t)$ is the likelihood of the camera and the 3D objects at time step $t$, which can be modeled with 3D object detection methods. $P(I_t | C_t, G_t)$ is the likelihood of the ground plane at time step $t$, where cues of the ground plane from the image can be used for evaluation.

\subsubsection{Camera Likelihood}

Estimate the camera parameters $C'_t$ with incremental bundle adjustment \cite{snavely2008modeling}. The camera likelihood $P(I_t | C_t, I^{t-1})$ can be computed according to the error between $C'_t$ and $C_t$:
\begin{equation}\label{camera}
    P(I_t | C_t, I^{t-1}) \propto \exp(-\| C_t - C'_t \|^2).
\end{equation}

\subsubsection{Object Likelihood}

The object likelihood $P(I_t | C_t, O_t)$ is modeled with the detection responses from a 3D object detector. The 3D object detector adapts its behavior according to occlusion between objects. In order to keep identity of object, online learning methods for object appearances can also be employed.

\subsubsection{Ground Plane Likelihood}

For the ground plane likelihood $P(I_t | C_t, G_t)$, we first compute the vanishing point $V'_t$ according to the visual features in frame $I_t$, and the vanishing point $V_t$ by projecting the ground plane $G_t$ according to the camera $C_t$. Then the ground plane likelihood can be computed as
\begin{equation}\label{ground}
    P(I_t | C_t, G_t) \propto \exp(-\| V_t - V'_t \|^2).
\end{equation}
Vanishing point for scene modeling is also used in \cite{geiger2011joint}.

\subsection{Motion Model}

The motion model $P(X_t | X_{t-1})$ predicts the current state of the scene based on its previous state. According to the components in the scene, we decompose the motion model as
\begin{align}\label{eq:motion}
& P(X_t | X_{t-1}) = P(C_t, O_t, G_t | C_{t-1}, O_{t-1}, G_{t-1}) \nonumber \\
& \propto P(C_t | C_{t-1}) P(O_t | O_{t-1}, G_t) P(G_t | G_{t-1}), 
\end{align}
where $P(C_t | C_{t-1})$ is the motion model for the camera, $P(O_t | O_{t-1}, G_t)$ is the model model for the objects which also incorporates physical constraints between the scene elements, and $P(G_t | G_{t-1})$ updates the ground plane estimation sequentially.

\subsubsection{Camera Motion}

We use constant velocity motion model for the translation of the camera. We introduce Gaussian noisy to the three angles of the camera rotation matrix.

\subsubsection{Object Motion}

We use constant velocity motion model for the positions of objects on the ground. We also introduce physical constraints in the object motion model.

\subsubsection{Ground Plane Updating}

We introduce Gaussian noise to the position of the ground plane.

\section{Experiments}


\section{Conclusion}


\bibliographystyle{ieee}
\bibliography{egbib}

\end{document} 
